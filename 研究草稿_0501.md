# 基于图神经网络与多智能体强化学习的移动策略学习以增强空间公共物品博弈中的合作

## 摘要

在多智能体系统（MAS）中促进合作的涌现与维持是一个核心挑战，尤其是在公共物品博弈（PGG）所模拟的、个体存在“搭便车”诱惑的情境中。虽然已知空间结构能通过空间互惠机制促进合作，但智能体**移动性**（Mobility）的作用，特别是当其超越简单的预设规则时，仍有待深入理解。现有模型常采用简单或反应式的移动规则，未能捕捉智能体如何基于复杂的社会环境进行**策略性空间定位（Strategic Positioning）**以主动促进合作。本项目提出了一个创新框架：在一个连续空间的PGG环境中，智能体利用**基于图神经网络（GNN）的多智能体强化学习（MARL）** 来学习复杂的**移动策略**。智能体的博弈策略（合作/背叛）仍通过模仿（费米规则 Fermi Rule）进行演化，但其**移动策略**则通过学习（使用 MAPPO/IPPO 及 GNN）来优化集体目标（如总收益或合作水平）。本研究旨在：（1）证明学习到的策略性移动能够显著增强合作水平，超越标准基准策略；（2）揭示由这些学习策略驱动的特定的**时空自组织机制**；（3）比较集体与个体学习目标下的效果差异。本研究旨在阐明**移动性作为一种可学习的社会策略**，并为设计能够内生合作的自主移动系统提供基础性见解。

------

## 1. 引言

在自利个体组成的系统中，合作行为如何演化是一个跨越生物学、经济学到人工智能等多个学科的基础性难题。公共物品博弈（PGG）是研究此类合作困境的经典模型。尽管空间结构和有限迁移已被证明能通过空间互惠促进合作，但现实世界中的多智能体系统（MAS）——从机器人集群、自动驾驶车队到在线社交网络用户——往往展现出更为复杂和**策略性的移动行为**。

然而，当前主流的空间博弈模型大多采用预设的、通常是简单的或反应式的移动规则（例如，随机游走、条件迁移等）。这些方法的一个**关键局限性**在于，它们无法捕捉智能体如何基于对局部社会环境（如邻居的策略、收益、移动等）的精细感知，进行**主动且策略性的空间定位**。这些预设规则内在地缺乏**远见（Foresight）**，无法根据对未来交互和收益的预期来优化当前位置，也难以适应动态变化的社会结构，因此未能充分探索移动性在塑造合作结果方面的全部潜力。

这就引出了一个基础且未被充分探索的问题：**智能体能否通过学习复杂的、目标导向的移动策略，来主动塑造有利于合作涌现和维持的社会交互网络？** 这个视角将移动性从单纯的扩散过程提升为一种**可学习的社会策略（Mobility as a Learnable Social Strategy）**。回答这个问题，需要从固定的移动规则转向基于经验学习的自适应策略。

为应对此挑战，本项目引入了一个创新框架，将**连续空间中的PGG交互**与前沿的**基于图神经网络（GNN）的多智能体强化学习（MARL）** 技术相结合。我们明确放弃预设移动规则，转而让智能体**学习**其移动策略。GNN特别适合处理MAS交互中固有的动态图结构信息（变化的邻居、复杂的局部特征）。MARL算法（特别是面向集体目标的MAPPO和面向个体目标的IPPO）为智能体学习优化移动策略以实现特定目标（例如最大化系统总收益或平均合作水平）提供了强大的范式。与此同时，智能体的PGG策略（合作/背叛）仍然通过标准的社会学习规则（基于收益比较的费米更新）演化，这使我们能够分离并研究**学习到的移动性**本身对合作的影响。

**因此，本研究的核心目标在于：**

1.  **验证“学习策略性移动增强合作”的核心假设：** 通过MARL学习到的移动策略，相比于基准移动规则（随机游走、静态、简单反应式规则），能够**显著且鲁棒地**提升空间PGG中的合作水平和社会总福利。
2.  **揭示合作涌现的新机制：** 识别、刻画并分析由学习到的移动策略所驱动的**特定的时空自组织模式**（例如，动态的合作者集群形成与维持、对背叛者的主动隔离、依据收益进行的战略性占位等），并阐明这些模式**如何**有效地克服搭便车问题。
3.  **为可设计的合作性移动MAS提供原则：** 通过比较不同学习目标（集体MAPPO vs. 个体IPPO）下的涌现策略和系统结果，为未来设计能够通过学习智能移动来**内生地**产生和维持合作行为的自主移动系统（如协作机器人、自组织通信网络）提供**原理性指导和可量化的策略特征**。

我们预期本研究不仅能加深对移动性、网络结构与合作演化之间复杂相互作用的理解，还将开创利用先进AI技术（MARL与GNN）主动引导复杂适应性系统中合作行为涌现的新途径。

------

## 2. 框架描述

### 2.1 系统设置

*   **空间:** $L \times L$ 的二维连续正方形区域，具有周期性边界条件。
*   **智能体:** $N$ 个同质智能体 $\{1, 2, \dots, N\}$。
*   **时间:** 离散时间步 $t=0, 1, 2, \dots, T$。

### 2.2 智能体状态

*   **物理状态:** 智能体 $i$ 在时间 $t$ 具有位置 $\mathbf{x}_i(t) \in [0, L]^2$ 和速度**方向**向量 $\mathbf{v}_i(t) \in \mathbb{R}^2$ (归一化, $\|\mathbf{v}_i(t)\| = 1$)。速率恒定为 $v > 0$。
*   **博弈状态:** 智能体 $i$ 在时间 $t$ 持有博弈策略 $a_i^g(t) \in \{C, D\}$ (C=合作, D=背叛) 和上一时间步 $t-1$ 的总PGG收益 $\mathcal{R}_i(t-1)$。

### 2.3 智能体观测 (学习移动策略 $\pi_i^m$ 的输入)

智能体 $i$ 用于其**移动策略**的观测 $o_i^m(t)$ 包括：

*   **自身信息:**
    *   当前速度方向 $\mathbf{v}_i(t)$。
    *   归一化位置 $\mathbf{x}_i(t)/L$。
    *   上一轮总收益 $\mathcal{R}_i(t-1)$。
    *   当前博弈策略 $a_i^g(t)$。
*   **邻域信息 (感知/交互半径 $r$ 内):** 对于每个邻居 $j \in \mathcal{N}_i(t) = \{k | \|\mathbf{x}_k(t) - \mathbf{x}_i(t)\| \le r, k \ne i\}$：
    *   相对位置: $\Delta \mathbf{x}_{ji}(t) = \mathbf{x}_j(t) - \mathbf{x}_i(t)$。
    *   邻居速度方向: $\mathbf{v}_j(t)$。
    *   邻居上一轮总收益 $\mathcal{R}_j(t-1)$。
    *   邻居当前博弈策略 $a_j^g(t)$。
    *   *(可选: 相对速度 $\Delta \mathbf{v}_{ji}(t)$)*

### 2.4 移动动态 (通过RL学习)

1.  在每个时间步 $t$，智能体 $i$ 根据其学习到的移动策略 $\pi_i^m$ 和观测 $o_i^m(t)$ 选择下一时刻的速度方向 $\mathbf{v}_i(t+1)$:
    $\mathbf{v}_i(t+1) = \pi_i^m(o_i^m(t))$ (策略网络输出归一化方向)
2.  位置根据恒定速率 $v$ 更新：
    $\mathbf{x}_i(t+1) = (\mathbf{x}_i(t) + v \cdot \mathbf{v}_i(t+1)) \pmod L$

**RL 动作空间 (移动策略 $\pi_i^m$ 的输出)**:

*   **连续动作:** 输出一个二维向量代表目标速度方向 $\mathbf{v}_i(t+1)$，通常通过策略网络的 `tanh` 激活函数确保分量在[-1, 1]内，然后进行归一化。

### 2.5 公共物品博弈 (PGG) 机制

在每个时间步 $t$ 的移动**之后**进行：

1.  **交互小组:** 每个智能体 $i$ 识别半径 $r$ 内的邻居 $\mathcal{N}_i(t)$。
2.  **多场博弈:** 每个智能体 $i$ 同时参与 $1 + |\mathcal{N}_i(t)|$ 场PGG：一场以自己为中心 ($G_i = \{i\} \cup \mathcal{N}_i(t)$)，以及它作为成员参与的、以每个邻居 $j \in \mathcal{N}_i(t)$ 为中心的博弈 ($G_j = \{j\} \cup \mathcal{N}_j(t)$)。
3.  **单场博弈规则:** 对于以智能体 $j$ 为中心的博弈 (参与者集合 $G_j$)：
    *   合作者 ($k \in G_j, a_k^g(t)=C$) 贡献成本 $c=1$。背叛者贡献 0。
    *   令 $n_C^{(j)}(t)$ 为 $G_j$ 中的合作者数量, $n_g^{(j)}(t) = |G_j|$ 为总人数。
    *   总贡献 $c \cdot n_C^{(j)}(t)$ 被乘以增强因子 $r > 1$。
    *   产生的总收益 $r \cdot c \cdot n_C^{(j)}(t)$ 在所有 $n_g^{(j)}(t)$ 个参与者中**平均分配**。
4.  **单场收益计算:** 智能体 $i$ ($i \in G_j$) 从以 $j$ 为中心的博弈中获得的收益 $\pi_i^{(j)}(t)$ 为：
    $\pi_i^{(j)}(t) = \frac{r \cdot c \cdot n_C^{(j)}(t)}{n_g^{(j)}(t)} - c \cdot \mathbb{I}(a_i^g(t)=C)$
5.  **总收益计算:** 智能体 $i$ 在时间步 $t$ 的总收益 $\mathcal{R}_i(t)$ 是其参与的所有博弈（自己发起的和作为成员参与的）的收益之和：
    $\mathcal{R}_i(t) = \sum_{j \text{ s.t. } i \in G_j} \pi_i^{(j)}(t)$
    *(对所有包含智能体 i 的博弈小组 $G_j$ 进行求和)*

### 2.6 博弈策略更新 (费米规则 Fermi Rule - 非RL学习)

在时间步 $t$ 结束时进行，决定 $a_i^g(t+1)$：

1.  智能体 $i$ 从其邻居集合 $\mathcal{N}_i(t)$ 中**随机**选择一个邻居 $j$（如果存在）。
2.  计算两者在时间步 $t$ 的**总收益差**: $\Delta \mathcal{R}_{ji} = \mathcal{R}_j(t) - \mathcal{R}_i(t)$。
3.  智能体 $i$ 以概率 $P_{i \leftarrow j}$ 采纳邻居 $j$ 的博弈策略 $a_j^g(t)$ 作为自己的下一时刻策略 $a_i^g(t+1)$：
    $P_{i \leftarrow j} = \frac{1}{1 + \exp\left( - \Delta \mathcal{R}_{ji} / \beta \right)}$
    其中 $\beta$ 是选择强度参数（控制噪声水平，$\beta$越大模仿确定性越强）。若无邻居，则策略不变。

*此规则模拟基于成功的社会模仿，其驱动力（收益差）受到学习到的移动策略的间接影响。*

------

## 3. 通过MARL学习移动策略

### 3.1 强化学习目标

核心目标是学习智能体的移动策略 $\{\pi_i^m\}$，通过在一个训练回合（episode, 例如 T=50 或更多步）内优化选定的RL奖励信号，来**间接**地促进系统层面的合作

**RL 奖励信号 ($R(t)$):**

*   **MAPPO (CTDE):** 使用共享的全局奖励信号，该信号源自优化目标，Critic 致力于优化平均系统收益 $R_{\text{global}}(t) = \frac{1}{N} \sum_{j=1}^N \mathcal{R}_j(t)$ ，而 Actor 则根据观测来分散式执行

### 3.2 多智能体强化学习算法

*   **MAPPO (Multi-Agent PPO):** 采用集中式训练、分散式执行（CTDE）的范式。中央评论家（Critic）使用全局状态信息（或经GNN聚合的信息）来估计价值函数，指导分散的行动者（Actor）的训练，而行动者基于局部（经GNN处理的）观测进行决策。适合优化集体目标。

### 3.3 用于观测处理的GNN架构

GNN对于使智能体能够处理来自其可变大小邻域的复杂动态信息至关重要。

*   **输入:** 智能体观测 $o_i^m(t)$ 被构造成图数据：
    *   **节点特征:** 包含智能体 $i$ 自身信息的向量。
    *   **图结构:** 由邻居关系（距离 $\le r$）定义。`edge_index` 记录连接。
    *   **边特征:** 包含每个邻居 $j \in \mathcal{N}_i(t)$ 的相对位置、速度、收益、策略等信息的向量。
*   **网络:** 使用 PyTorch Geometric 等库实现。
    *   对输入的节点/边特征进行嵌入。
    *   多层GNN（如图注意力网络 GATConv、图卷积网络 GCNConv 或 TransformerConv）通过消息传递聚合邻域信息，更新每个智能体的节点嵌入。
    *   这些嵌入作为 Actor (策略网络) 和 Critic (价值网络) (通常是MLP) 的输入。GNN使策略能够基于丰富的社会环境信息进行决策。
*   **输出:** 动作概率（离散动作）或分布参数（如高斯分布的均值和标准差，用于连续动作 - 速度方向）。对于MAPPO的Critic，可能需要通过对节点嵌入进行池化（pooling）来获取全局信息。

------

## 4. 实验设计

### 4.1 基准策略 (Baselines)

将MARL学习到的策略与以下基准进行比较：

1.  **随机游走 (Random Walk, RW):** 智能体每步随机选择方向。博弈策略按费米规则更新。
2.  **静态网络 (Static):** 智能体位置固定。博弈策略按费米规则更新。
3.  **简单反应式规则 (例如，类Vicsek模型):** 智能体尝试与邻居速度方向对齐（可加噪声）。博弈策略按费米规则更新。（也可测试基于收益的简单规则，如朝向高收益邻居移动）。

### 4.2 参数设置与敏感性分析

*   PGG乘子 $r$ (影响合作可行性的关键参数)

### 4.3 评价指标

除标准RL指标（奖励、回合长度）外，更关注：

1.  **合作水平:** 时间平均的合作者比例 $\langle f_C \rangle$。
2.  **社会福利:** 时间平均的系统总收益 $\langle \mathcal{R}_{total} \rangle$。
3.  **收益不平等性:** 收益的基尼系数或标准差。
4.  **空间结构与动态:**
    *   **聚类系数:** 智能体的平均局部聚类系数，可区分 C-C, C-D, D-D 对。
    *   **径向分布函数 (RDF):** $g(d)$，用于量化 C-C, C-D, D-D 对之间的空间分离/聚集程度。
    *   **团簇分析:** 合作者团簇的大小分布和生命周期。
    *   **网络指标 (基于交互图):** 度分布、模块度随时间的变化。
5.  **合作稳定性:** 系统达到稳态（如果存在）所需的时间，以及稳态附近的波动性。

### 4.4 核心研究问题 (修订版)

**在一个智能体可以移动的空间公共物品博弈中，当智能体与自身邻域范围内的k个最近邻居进行交互，且其博弈策略（合作/背叛）会根据基于长期适应性收益的Fermi规则进行演化时，智能体为了最大化自身收益，会通过强化学习（GNN+MAPPO）学会怎样的移动策略？以及，这些学到的移动策略是如何促进或抑制合作的涌现与维持的？**

1.  **有效性与鲁棒性:** 与基准策略相比，基于GNN的MARL学习到的策略性移动能在多大程度上提升PGG系统的**合作均衡水平**和**社会总福利**？这种提升在不同的博弈参数（$r, \beta$）和系统密度下是否**鲁棒**？
2.  **机制识别:** 学习到的“合作促进型”移动策略展现出哪些**可识别的时空模式**（例如，动态聚类、主动隔离、边界巡逻、追随/逃离特定类型邻居）？这些模式如何被**量化**（例如，通过网络指标、轨迹分析）？这些模式与合作水平的提升之间存在怎样的**因果关联**？
3.  **策略的适应性:** 学习到的最优移动策略是否会随着PGG博弈参数（$r, \beta$）或环境参数（密度, 交互半径 $r$）的变化而**自适应地改变**？这种适应性是如何体现的？
4.  **学习目标的影响 (集体 vs. 个体):** MAPPO（优化集体目标）和IPPO（优化个体目标）学习到的移动策略在**结构和效果**上有何本质区别？个体优化是否也能意外地导致集体合作的提升，其机制是什么？哪种范式更能产生**可持续且公平**的合作状态？
5.  **GNN表征的作用:** GNN在学习过程中**提取了哪些关键的局部社会博弈信息**来指导有效的移动决策？（例如，通过注意力权重、特征重要性分析等）。这种表征如何使能策略性行动？

------

## 5. 预期贡献

*   **概念提升:** 在合作演化研究中确立**移动性作为一种可学习的社会策略**的视角，超越预设移动规则的局限。
*   **方法创新:** 提出并验证一个专为学习空间社会困境中策略性移动而设计的**基于GNN的MARL框架**。
*   **机制发现:** 揭示学习到的移动性促进PGG中稳健合作的潜在**新的时空自组织机制**。
*   **比较性洞见:** 提供关于**集体与个体学习目标**如何塑造移动策略及合作结果的系统性比较。
*   **设计启示:** 为未来设计能够通过智能移动**内生地学习**以实现和维持合作的自主系统（如机器人团队、网络化智能体）提供**AI驱动的原理和可量化的见解**。



## 7. 创新点讨论

**有可能**最终观察到的宏观现象（例如，合作者倾向于聚集、合作水平得到提升）与使用更简单模型（如 He et al. 的 Q-learning 或其他反应式规则）得到的结果在**表面上看起来相似**。**但是，即使宏观现象相似，通过MARL训练得到的“新见解”很可能体现在以下几个层面，这些是简单模型难以揭示的：**

1. **机制的复杂性与适应性 (Complexity and Adaptivity of the Mechanism):**
   - **简单模型:** 合作增强通常归因于相对**静态或简单的机制**，如“空间互惠”（合作者抱团抵抗入侵）或简单的趋利避害规则（靠近高收益邻居）。移动规则通常是固定的或基于非常有限状态的反应。
   - **MARL可能的新见解:**
     - **动态的时空策略:** MARL可能学习到**远比静态聚类更复杂的动态移动策略**。例如，智能体可能学会**根据环境变化（如邻近区域的策略/收益波动）动态调整聚集程度**，或者学会在**不同阶段（如探索期 vs. 稳定期）采用不同的移动模式**。
     - **预见性/规划性移动 (Anticipatory/Proactive Movement):** 基于Actor-Critic框架和对未来价值的估计，MARL智能体可能学会**预见性地移动**。它们可能不是等到周围环境变差才移动，而是根据GNN处理的邻域信息**预测**到潜在威胁（如靠近的背叛者群）或机会（如新形成的合作者空隙）而**提前布局**。这是简单Q-learning或反应规则难以做到的。
     - **更精细的社会互动策略:** 移动本身可能成为一种**更复杂的社会信号**。例如，智能体可能学会根据邻居的具体行为（不仅仅是C/D策略，还可能是其移动模式或历史收益）来决定是靠近、远离、跟随还是进行某种形式的“驱赶”或“邀请”式移动。
2. **信息处理与决策依据 (Information Processing and Decision Basis):**
   - **简单模型:** 决策依据通常是简化的局部信息（如邻居策略比例、与某个随机邻居的收益比较）。
   - **MARL可能的新见解:**
     - **GNN揭示关键信息:** 通过分析训练好的GNN（例如，使用注意力权重、特征重要性分析等技术），我们可以**量化地揭示**智能体在做移动决策时，**到底依赖邻域中的哪些信息**？是邻居的相对位置更重要，还是它们的策略、收益、或者甚至是邻居的邻居（二阶邻域信息，如果GNN层数足够）？不同情况下（如高密度 vs. 低密度，高收益梯度 vs. 低收益梯度）依赖的信息是否不同？这能提供关于智能体如何“理解”其社会环境的深刻见解。
     - **上下文依赖的决策逻辑 (Context-Dependent Logic):** MARL策略网络是一个复杂的非线性函数。智能体可能学会在**不同的社会上下文（Context）下，对相同的局部刺激做出不同**的移动反应。例如，当全局合作水平高时，遇到一个背叛者邻居可能采取容忍或缓慢远离的策略；而当全局合作水平低时，则可能采取快速逃离或主动隔离的策略。这种复杂的上下文依赖性是简单规则难以模拟的。
3. **集体策略与个体策略的权衡 (Trade-offs between Collective and Individual Strategies):**
   - **简单模型:** 通常难以区分是纯粹的个体自利行为恰好导致了集体好的结果，还是存在某种形式的隐性协调。
   - **MARL可能的新见解:**
     - **MAPPO vs. IPPO的直接比较:** 这是你研究设计的核心优势之一。通过直接对比优化集体目标（MAPPO）和优化个体目标（IPPO）所学习到的移动策略，你可以清晰地看到差异。**MAPPO是否学习到了IPPO无法学到的、真正需要牺牲个体短期利益来实现集体合作的移动策略？** 例如，某些智能体是否学会了扮演“守卫者”角色，停留在集群边缘以阻止背叛者入侵，即使这个位置本身收益不高？或者是否学会了某种“利他主义”的移动，去帮助被围困的合作者？这种**明确的集体导向策略**的证据将是全新的见解。
4. **涌现的组织模式的多样性 (Diversity of Emergent Organizational Patterns):**
   - **简单模型:** 往往只观察到相对单一的模式，如紧密抱团。
   - **MARL可能的新见解:** 由于策略的复杂性和适应性，MARL可能会产生**更多样化、更精细的自组织结构**。例如，可能出现核心-边缘结构、动态的通道或边界、甚至是在移动模式上出现某种程度的**角色分化**（即使智能体是同质的）。识别和理解这些由复杂学习策略驱动的新模式本身就是重要见解。

**总结来说，即使最终都“促进了合作”，MARL带来的新见解在于：**

- **揭示更复杂、动态、预见性的合作促进机制:** 不再仅仅是简单的抱团取暖，而是智能体如何通过学习灵活移动来主动管理风险、利用机会、甚至可能进行策略性的社会互动。
- **理解智能体如何处理和利用复杂的社会信息:** GNN让我们能“打开黑箱”，看看智能体到底学到了什么决策依据。
- **厘清个体与集体目标在塑造移动策略中的作用:** MAPPO vs. IPPO的对比能直接回答个体优化与集体优化在移动策略上的本质差异。
- **发现由学习驱动的、可能更多样化的自组织模式:** 超越简单的聚类，探索更丰富的时空结构。

所以，你的研究重点不应仅仅是“合作水平是否提高”，而更在于**深入分析学习到的移动策略本身的性质、驱动这些策略的信息处理机制、以及它们所产生的（可能是新的）时空组织模式**。这才是利用先进MARL方法能带来的独特价值和新见解。你需要设计好实验后的**策略分析**和**行为分析**环节，才能充分挖掘这些潜在的创新点。



核心 insight 可以聚焦于以下几点：

1. **移动性作为双刃剑**: 智能体通过学习移动策略确实可以提升个体（或平均集体）的博弈收益，但这**并不自动等同于合作率的提升**。学习到的移动策略可能更侧重于**优化个体的博弈参与效率和风险规避**（例如，更有效地找到并利用现有合作者，或避免被剥削），而不是直接促进新的合作行为产生或维持。
2. **GNN-MARL 学习到的策略性定位与简单规则的差异**: 即使最终的合作率可能与某些简单规则下的结果相似（或者在某些参数区间内），GNN-MARL 学习到的移动策略在**信息利用的复杂性、对环境动态的适应性、以及可能的预见性方面**，应该展现出与简单规则（如随机游走、静态、简单反应式规则）的本质区别。我们需要揭示这些“智能化”移动策略的具体模式。
3. **集体目标  vs. 个体目标  对移动策略和合作涌现的影响**:
   - 如果 以**集体收益或合作率为奖励** 能够比 **以个体收益为奖励** 学出更显著提升合作率的移动策略，那么这将证明**明确的集体目标引导对于通过移动性促进合作至关重要**。
   - 如果两者结果相似，或者通过集体引导仍难以大幅提升合作率，则进一步说明问题的复杂性，可能需要更精巧的集体奖励设计或机制。
4. **时空自组织模式的新发现**: 学习到的移动策略可能会驱动智能体形成一些**新的、非平凡的动态空间组织模式**（例如，流动的合作者-背叛者边界、追逃动态、特定角色的临时涌现等），这些模式是简单规则难以产生的，并且可能对理解合作的维持或崩溃有重要意义。

## 8. 实验组织

**第一组图：MAPPO 性能基准与不同奖励信号下的初步探索**

- **小节 4.1: 基于平均收益奖励的 MAPPO 性能评估 (N=100)**
  - **图片 4.1.1: MAPPO (平均收益奖励) vs. 基准策略的合作率与收益 (N=100)**
    - **子图 (a): 合作率 vs. 评估步数**
      - 曲线1: **MAPPO (奖励 = 平均系统收益)** - 均值 + 标准差阴影
      - 曲线2: Random Walk (RW) - 均值 + 标准差阴影
      - 曲线3: Static - 均值 + 标准差阴影
    - **子图 (b): 平均个体收益 vs. 评估步数**
      - 包含与 (a) 相同的策略。
    - **分析逻辑**:
      - 在 N=100 的更大规模系统下，验证之前观察到的现象（MAPPO 在平均收益奖励下，收益可能提升，但合作率不一定）。
      - 与基准进行比较，评估当前 MAPPO 策略的相对性能。
      - 讨论这种奖励信号下，算法可能学习到的策略倾向（如更高效的个体博弈参与）。
    - **需要的实验**:
      - 调整参数以在 N=100, L=20 的设置下稳定运行 MAPPO（奖励=平均收益）。
      - 运行足够长的训练。
      - 对 MAPPO、RW、Static 进行充分的评估。
- **小节 4.2: 探索直接优化合作率的 MAPPO 性能 (N=100)**
  - **图片 4.2.1: MAPPO (合作率奖励) vs. 基准策略的合作率与收益 (N=100)**
    - **子图 (a): 合作率 vs. 评估步数**
      - 曲线1: **MAPPO (奖励 = 全局合作率)** - 均值 + 标准差阴影
      - 曲线2: Random Walk (RW) - 均值 + 标准差阴影
      - 曲线3: Static - 均值 + 标准差阴影
      - *(可选加入: MAPPO (奖励 = 平均系统收益) 以直接对比)*
    - **子图 (b): 平均个体收益 vs. 评估步数**
      - 包含与 (a) 相同的策略。
    - **分析逻辑**:
      - 当 RL 的直接优化目标是合作率时，MAPPO 是否能显著提升合作水平？
      - 这种直接优化合作率的策略，对平均个体收益有何影响？是否存在为了集体合作而牺牲个体短期收益的情况？
      - 与优化平均收益的 MAPPO 策略进行对比，揭示不同奖励信号对学习策略和系统行为的塑造作用。
    - **需要的实验**:
      - 实现一个新的训练运行，其中 MAPPO 的奖励信号修改为全局合作率。
      - 运行足够长的训练。
      - 进行充分评估。

**第二组图：深入分析学习到的移动策略与时空模式 (N=100)**

在第一组图的基础上，如果我们观察到某些 MAPPO 策略（例如，直接优化合作率的 MAPPO）确实表现出了一些有趣的合作增强效果，或者即使没有，我们也需要理解学习到的移动策略到底是什么样的。

- **小节 4.3: 学习到的移动策略驱动下的空间自组织模式分析**
  - **图片 4.3.1: 不同策略下的典型快照与智能体空间分布**
    - **子图 (a1, a2, a3...):** MAPPO (平均收益奖励) 下不同评估时间点的系统快照。用不同颜色标记合作者和背叛者。
    - **子图 (b1, b2, b3...):** MAPPO (合作率奖励) 下不同评估时间点的系统快照。
    - **子图 (c1, c2, c3...):** Random Walk 下不同评估时间点的系统快照。
    - **分析逻辑**:
      - 直观展示不同策略下智能体的空间聚集、分散、集群形成等模式。
      - 比较学习到的 MAPPO 策略与 RW 在空间组织上的差异。
      - 寻找是否有特定的、与高合作（或高收益）相关的空间结构。
    - **需要的实验**: 在评估过程中，定期保存系统的完整状态（智能体位置、博弈策略）以供后续可视化。
  - **图片 4.3.2: 量化空间结构的指标比较**
    - **子图 (a): 平均局部聚类系数 vs. 时间 (或 PGG 乘子 r)**。比较不同 MAPPO 策略和 RW。
    - **子图 (b): 径向分布函数 (RDF) g(d) - C-C, C-D, D-D 对**。选择有代表性的稳态进行比较，展示不同策略下不同类型智能体对的聚集/分离程度。
    - **(可选子图 c): 合作者团簇大小分布**。比较不同策略下形成的合作者团簇的特征。
    - **分析逻辑**:
      - 用量化指标来支撑对空间模式的观察。
      - 例如，如果合作率高的 MAPPO 策略展现出更高的 C-C 聚类或特定的 RDF 模式，这将是机制上的证据。
    - **需要的实验**: 在评估或特定参数的模拟运行中，计算这些网络/空间统计指标。
- **小节 4.4: (如果适用) 奖励塑造对移动策略特征的影响**
  - **图片 4.4.1: 移动策略特征分析 (例如，平均移动距离、方向变化、对特定邻居的反应)**
    - 这个图可能不是单一的曲线图，而是几种不同的统计图或分布图。
    - 例如，比较在“MAPPO (平均收益奖励)”和“MAPPO (合作率奖励)”下，智能体在遇到不同邻域情况（如周围全是C，全是D，或混合）时的平均移动幅度或方向选择的差异。
    - **分析逻辑**:
      - 尝试从更微观的层面理解不同奖励信号如何塑造了智能体的移动决策逻辑。
      - 是否“合作率奖励”下的 MAPPO 学习到了更倾向于保护合作者或隔离背叛者的移动模式？
    - **需要的实验**: 这部分可能需要更复杂的轨迹分析和数据挖掘。可以在获得显著结果的 MAPPO 策略上进行。





