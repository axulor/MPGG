# Multi-Agent Q-Learning Dynamics in a Migratory Public Goods Game



## 1.introduction 

进化博弈的研究表明，在重复交互或适当激励机制的作用下，合作策略会有机会在社会困境的群体中涌现。早期的研究大多都建立在静态网络拓扑（尤其是晶格网络）的强假设之上。现实中多智能体所处的交互环境往往具有高度的动态性与异质性，仅依靠固定的邻里结构或单一的短期收益评估，往往难以有效地促进可持续的合作。一些研究通常尝试通过邻居重连机制来模拟这样的动态场景：允许智能体通过切断与背叛者的连接、建立新链接来优化局部交互环境，形成促进合作的策略聚类。然而，尽管邻居重连机制在静态网络中展现出促进网络互惠性的潜力，但却难以应用于无人机集群、蚁群蜂群等系统高动态场景中的合作维持问题。在这类系统中，当社会链接以高频率和大规模重构时，基于固定邻居关系的策略学习机制将面临根本性挑战。

上述挑战的根本来源于环境的高度异质性，在高动态场景中，社会链接频繁地断裂和重塑，使得各个智能体对博弈对手的观测呈现出显著的时空差异。同时，我们默认博弈规则这个隐藏的环境因素在全局范围内是一致、固定的。然而，真实系统的环境往往在资源分布、风险水平或其他真实因素方面呈现明显的不均匀性，这些异质性可以用到博弈规则的收益矩阵加以量化。因此，可以这么不失一般化的假设：真实动态系统中的环境异质性往往同时包含**交互对象的选择**与**博弈规则的空间差异**——例如，某些情景可能天然具备更高的合作回报率，而另一些情景则因成本过高抑制合作。在这种情况下，理性的智能体不仅需要应对对手策略的多样化，还必须适应不同空间位置或情境下的收益差异。基于网络结构的邻居选择机制仅能被动规避“坏对手”，却无法主动利用“好环境”；若将环境参数映射为空间可探索的变量，智能体便可通过迁移同时实现**对手选择**与**环境优化**的双重目标，从而在更广义的异质性中寻求合作优势。

现有多智能体系统的研究表明，相较于纯粹的社会学习（如策略模仿），基于强化学习的试错学习机制在合作行为的涌现中发挥了关键作用。在长时间和大空间尺度的演化过程中，智能体若反复背叛，最终会陷入收益孤立；相反，通过建立并维持合作团体则能获得更高的长期回报。与单纯的社会模仿机制相比，基于试错累积经验的强化学习能使智能体在复杂环境中自主发现并巩固合作策略，尤其在存在重复交互或长远收益考量时，这种试错过程更具优势。已有的研究在强调时间维度（如通过**记忆深度**或**折扣因子**调控长期回报）的同时，却普遍忽视了强化学习试错机制中**空间探索**的潜在价值。在断边重连的强化学习框架下，智能体通过策略迭代优化动作选择（如合作/背叛），但其试错过程通常局限于**固定环境参数下的交互关系调整**。这种局限导致智能体难以主动利用环境异质性——例如，在公共物品博弈中，若乘数因子（r）或成本（c）随空间位置变化，仅依赖时间维度的策略更新（如记忆历史对手行为）无法有效识别高回报区域的分布规律。因此，现有研究对空间维度的忽视实质上限制了智能体在**环境结构认知**与**自适应迁移决策**上的能力，而这恰是强化学习试错机制在广义异质场景中应拓展的核心维度。鉴于此，我们需要重新审视智能体的探索维度——**当环境参数本身具有空间异质性时，将位置选择纳入试错过程可能比单纯调整社交关系更具适应性优势**。

基于上述分析，本研究创新性地将博弈环境的关键参数映射到空间坐标中，构建了一个**具有周期性边界条件**的二维公共物品博弈场景，从而更加直观地考察智能体对博弈环境的试错与探索。具体而言，我们沿横轴从0到1定义公共物品博弈的乘数因子，沿纵轴从0到1定义成本因子，并据此将研究区域划分为25个离散的小方格“赌场”，每个赌场都对应不同的博弈参数设置。每个时间步里，同一赌场内的智能体共同参与公共物品博弈：合作方需支付其对应的成本因子，背叛方则无须成本。随后，智能体根据获得的收益（payoff）更新Q值，再决定下一步的移动位置，以在更广阔的空间维度中不断试错。通过这种设计，智能体在动态交互和自我学习过程中能够自发发现潜在的时空合作结构，为研究多智能体在高动态社会关系与异质环境下的合作涌现提供了一个新的实验框架。

该设计突破传统框架的三个方面：第一，将环境参数从外生常量转化为可探索的内生状态，使智能体能够主动寻找合作友好型区域；第二，建立策略与空间的耦合学习机制，Q值函数同时编码策略收益和位置效用；第三，通过参数梯度设计，可量化环境异质性对合作相变的影响。实验重点观测两个涌现特征：(i) 自组织形成的"参数走廊"现象：智能体群体是否向高r/c区域定向迁移；(ii) 策略-空间协同效应：当局部背叛者密度超过阈值时，合作者能否通过迁移维持生存优势。

————————————对引言的补充——————————————————

**在高动态、部分可观测且环境参数存在空间异质性的多智能体系统中，合作行为如何通过同时探索策略与空间位置而自发涌现和得以维持**。为此，他们将公共物品博弈的关键参数（乘数因子 $r$ 与成本因子 $c$）以二维坐标的形式映射到“空间位置”，并让智能体在该空间中不断通过试错（强化学习）来调整自身的策略和位置选择，从而研究以下核心问题：

1. **环境异质性与合作的关系**
   通过将环境参数从外生常量转化为可在空间中主动探索的“内生状态”，考察智能体如何在不同参数区（“赌场”）中发现并利用对合作更友好的区域。
2. **策略与空间耦合学习的影响**
   在传统的社会关系重连机制之外，作者进一步将位置选择纳入强化学习过程，使智能体的 Q 值函数同时考虑“策略收益”与“位置效用”，从而分析多智能体在高动态场景中如何自组织地涌现合作结构。
3. **合作涌现的时空机制**
   重点观测：
   - *(i)* 是否会出现“高 $r$ 低 $c$”参数区的“合作走廊”现象，即智能体群体向具有较高收益/低成本的区域定向迁移并形成稳定合作；
   - *(ii)* 当局部出现大规模背叛者时，合作者能否通过灵活迁移（在空间维度上逃离背叛高密度区域）来维持并重新获得生存优势。

简言之，要解决的具体问题就是：**在具有空间异质性和高动态交互的公共物品博弈环境中，如何利用强化学习的试错过程，使智能体同时探索策略和位置，从而推动并稳定群体层面的合作涌现。**

传统基于固定交互结构或预设社会规范的方法面临两大局限：其一，动态拓扑削弱了策略聚类对背叛者的隔离效果[Perc et al., 2013]；其二，移动策略与博弈策略的分离优化可能导致局部收益与群体协作目标的冲突[Lowe et al., 2017]。尽管部分研究尝试通过引入声誉机制或道德激励缓解上述问题[Anastassacos et al., 2020; Tennant et al., 2023]，但这些外部干预可能降低模型的普适性与自组织特性。

————————————————————————————————————



## 2. Mathematical model

#### 2.1 Migratory public goods game

在本节中，我们将形式化地定义可迁移公共物品博弈（Migratory Public Goods Game，MPGG）环境。整体博弈场域是一个具有无周期边界条件的正方形区域，边长设为 $L$。其中，横轴表示公共物品博弈中经调整后映射至 $[0,1]$ 区间的增益因子 $\alpha$，纵轴表示智能体进行一次博弈所需投入的固定成本 $c$，其取值同样位于 $[0,1]$。通过将该正方形区域沿横纵两个方向均匀划分边长为 $l$ 的方格，可得到总数为 $M=(L/l)^2$ 的离散子区域。我们将这些小方格称作“赌场”，并用离散索引 $m=(c,\alpha)$ 标识每一个赌场。

给定总智能体数为 $N$，在初始时刻将所有智能体均匀分布至各赌场，则每个赌场最初拥有的智能体数为
$$
n_m(0)=\left\lvert \frac{N}{(L/l)^2} \right\rvert.
$$

在任意时间步 $t$，若赌场 $m$ 内现存智能体数为 $n_{m,t}$，则该赌场内的所有智能体都会参与一轮公共物品博弈。智能体的博弈动作集记为 $A_G=\{C,D\}$。完成博弈后，每个智能体 $i$ 在时间步 $t$ 获得的收益记为 $\pi^i_t$。具体地，若智能体 $i$ 在赌场 $m$ 所选择的博弈动作为 $a^i_{m,t}$，则收益函数 $\pi^i_{m,t}$ 定义如下：
$$
\pi_{m,t}^{i}=
\begin{cases}
r\frac{c\cdot n_{{m,t}}^{C}}{n_{m,t}} - c, & a_{m}^{i}(t)=C,\\
r\frac{c\cdot n_{{m,t}}^{C}}{n_{m,t}}, & a_{m}^{i}(t)=D.
\end{cases}
$$

其中，$c \in [0,1]$ 为固定成本，$n_{m,t}^{C}$ 表示在时间步 $t$ 选择合作动作 $C$ 的智能体数量；增益因子 $\alpha \in [0,1]$ 则通过某种静态映射给定公共物品博弈中的实际放大倍数 $r(\alpha)$。值得注意的是，赌场内的实际人数  $n_{m,t}$ 会随时间演化，从而影响比值 $\tfrac{r}{n_{m,t}}$ 并进而左右合作形成的难易程度。与其在每次人口变动时重新设定 $r$，我们在此假设 $r(\alpha)$ 为环境的固有属性，以更好地模拟真实系统中的自组织效应。

鉴于传统公共物品博弈（PGG）的结论，若在初始时刻赌场内智能体数量 $n_m(0)$ 固定，则对于某给定赌场，当 $1 < r < n_{m,0}$ 时通常会面临社会困境，而当 $r > n_{m,0}$ 时则合作占优。基于此，我们规定对某一确定的 $n_m(0)$，设
$$
1 < r_{\min} < n_{m,0} < r_{\max},
$$

并定义如下线性映射来获得增益因子：
$$
r(\alpha)=r_{\min}+\bigl(r_{\max}-r_{\min}\bigr)\alpha.
$$


**Remark**  

- 在低增益因子 $\alpha$ (即 $r(\alpha)\approx r_{\min}$) 的情况下：若赌场维持大规模玩家（$\ge n_{m,0}$)，则 $\tfrac{r(\alpha)}{n_{m,0}}\ll 1$，背叛方往往获得更高的相对收益，导致合作难以稳定。  
- 在高增益因子 $\alpha$ (即 $r(\alpha)\approx r_{\max}$) 的情况下：同样规模下 $\tfrac{r(\alpha)}{n_{m,0}} \ge 1$，合作更具优势，更易于维持。  
- 由于可迁移机制，智能体会选择逃离那些“人口密度过大且 $r$ 较小”的赌场，转而涌向“较高 $r$ 且人口适度”的高收益区域，促使各赌场人口发生再分布，并使得各赌场的社会困境强度进一步改变。

在随后的数值实验部分，我们将指定具体的 $n_{m,0}$、$r_{\min}$ 与 $r_{\max}$，从而在初始状态下为可迁移公共物品博弈的多智能体系统奠定不同的合作—背叛竞争格局。



#### 2.2 Multi-agent independent Q-learning

##### 2.2.1 部分可观测状态

在初始时刻，智能体被均匀分布到每个赌场，并随机选择动作进行一轮博弈。在时间步 $t$ 开始时，智能体 $i$ 观测到的局部信息作为博弈观测状态，记为 $o^i_{G,t}$， 被定义为如下所示的二元组：
$$
o^i_{G,t} = \left\langle m^i_t, n_{m,t},n_{m,t}^C \right\rangle
$$
其中：
- $m^i_t = (c,\alpha)$：智能体当前所在赌场的成本参数 $c$ 和增益因子 $\alpha$；
- $n_{m,t}$：当前赌场 $m^i_t$ 内的智能体总数；

在当前时间步 $t$ 进行一次博弈后，智能体 $i$ 能观测到的博弈后局部信息作为迁移观测状态，记为 $o^i_{M,t}$ 包括：
$$
o^i_{M,t} = \left\langle m^i_t, n_{m,t} ,n_{m,t}^C\right\rangle
$$
其在博弈观测状态 $o^i_{G,t}$ 的基础上新增 $n_{m,t}^C$  表示当前赌场 $m^i_t$ 内的合作智能体数量。

由于赌场的人数和策略在动态变化，且智能体无法观测到其他赌场的信息，也无法观测到其他玩家的具体策略和收益等信息，环境对于单个智能体而言是**非平稳**且**部分可观测**的。

##### 2.2.2 独立 Q-learning 更新机制

在独立 Q-learning（Independent Q-learning, IQL）框架下，每个智能体 $i$ 独立地学习两个 Q 函数，分别用于**博弈决策**和**迁移动作决策**。在整个学习过程中，智能体的目标是通过 Q-learning 估计最优的 Q 值函数，使得其累积折扣回报最大化。对于每个智能体 $i$，我们定义其**迁移动作的 Q 函数**和**博弈动作的 Q 函数**如下：
$$
Q^i_G(o^i_{G,t}, a_{G,t}^i)
$$
$$
Q^i_M(o^i_{M,t}, a_{M,t}^i)
$$

其中：

- $o^i_{G,t}$ 为智能体 $i$ 在时间步 $t$ 观测到的**博弈状态**，用于决定博弈动作；
- $o^i_{M,t}$ 为智能体 $i$ 在时间步 $t$ 观测到的**迁移状态**，用于决定移动策略；

- $a_G^i(t) \in A_G$ 为智能体 $i$ 在时间步 $t$ 选择的博弈动作；
- $a_M^i(t) \in A_M$ 为智能体 $i$ 在时间步 $t$ 选择的移动动作；

在**独立 Q-learning**框架下，每个智能体的两个 Q 表 $Q_G$ 和 $Q_M$ 都初始化为零。



##### 2.2.3 训练过程

整个 Q-learning 训练过程包括以下核心步骤：

**并行博弈阶段：更新 $Q_G$**

在每个时间步 $t$，每个智能体 $i$ 依据当前状态 $o^i_{G,t}$ 采取 $\epsilon$-贪婪策略选择博弈动作：
$$
a_G^i(t)=\begin{cases}\arg\max_aQ_G(o_G^i(t),a)&\text{以概率 }1-\epsilon\\\text{随机动作}&\text{以概率 }\epsilon&\end{cases}
$$
智能体执行该博弈动作，并获得即时奖励 $R^i_t$。同时，智能体观测到博弈后的新状态 $o^i_{G,t+1}$，并存储经验 $(o^i_{G,t}, a^i_{G,t}, R^i_t, o^i_{G,t+1})$ 进入经验回放池 $\text{buffer}_G$。

随后，从经验回放池中采样数据，利用 Q-learning 进行更新：

$$
Q^i_G(o^i_{G,t}, a^i_{G,t}) \leftarrow (1 - \eta) Q^i_G(o^i_{G,t}, a^i_{G,t}) + \eta \cdot \left[R^i_t+\gamma\max_{a^{\prime}}Q_G^i(o^i_{G,t+1}, a^i_{G,t+1})\right]
$$

其中：

- $\gamma$ 是折扣因子，衡量未来奖励的重要性；
- $\eta$ 是学习率，控制 Q 值更新的幅度。

**并行移动阶段：更新 $Q_M$**

在博弈阶段结束后，智能体根据当前状态 $o^i_{M,t}$ 采取 $\epsilon$-贪婪策略选择迁移动作：

$$
a_M^i(t)=\begin{cases}\arg\max_aQ_M(o_M^i(t),a)&\text{以概率 }1-\epsilon\\\text{随机动作}&\text{以概率 }\epsilon&\end{cases}
$$


然后，智能体执行该移动动作，并更新自身所在赌场位置 $m^i_{t+1}$，存储经验 $(o^i_{M,t}, a^i_{M,t}, o^i_{M,t+1})$ 进入经验回放池 $\text{buffer}_M$。

由于移动奖励是**延迟的**，智能体在 $t+1$ 时刻通过观测博弈收益 $R^i_{t+1}$ 来计算 Q 值更新目标。
$$
Q^i_M(o^i_{M,t}, a^i_{M,t}) \leftarrow (1 - \eta) Q^i_M(o^i_{M,t}, a^i_{M,t}) + \eta \cdot \left[R^i_{t+1}+\gamma\max_{a^{\prime}}Q_M^i(o^i_{M,t+1}, a^i_{M,t+1})\right]
$$


该过程使智能体能够学习到如何选择赌场，以优化其长期收益。

这里 $t$ 时刻的迁移奖励，实际来源于 $t+1$ 时刻的博弈收益，可以如下表示
$$
R_M(t)=E[\pi_{m^{\prime}}(t+1)|a_M(t)]
$$
**动态调整探索率**

为了实现探索与利用的平衡，采用指数衰减策略动态调整 $\epsilon$：

$$
\epsilon(t)=\epsilon_0\cdot e^{-\lambda t}
$$


其中，$\lambda$ 控制探索率的衰减速率，保证智能体在前期进行充分探索，而后期趋于稳定策略。

##### **经验回放机制**

为了提高学习的稳定性和效率，博弈和移动经验分别存储在两个**经验回放池** $\text{buffer}_G$ 和 $\text{buffer}_M$ 中。每个时间步，

- $\text{buffer}_G$ 清空，以便只保留最新的博弈数据；
- $\text{buffer}_M$ 只存储最新的移动经验，并在下个时间步使用。

经验回放机制使得 Q-learning 在**非平稳环境**下依然能够有效收敛。双缓冲池设计分别处理即时奖励（博弈）与延迟奖励（迁移）的不同学习特性，通过差异化采样策略提升数据效率。



采用双缓冲池实现时序解耦，博弈经验在动作执行后立即存储，同时等待下一时刻的新博弈观测信息，而迁移经验需等待下一时刻的奖励生成和新的博弈后新迁移信息。经验存取过程满足：
$$
\mathcal{B}_G(t)\ni(o_G^{t-1},a_G^{t-1},R^{t-1},o_G^t)\\\mathcal{B}_M(t)\ni(o_M^{t-1},a_M^{t-1},R^t,o_M^t)
$$


下列算法伪代码展示了智能体在 MPGG 环境中使用独立 Q-learning 进行学习的基本流程：

- ```latex
  # 初始化阶段
      为每个智能体 i 初始化 Q_M^i 和 Q_G^i 为零表;
      均匀分配智能体到赌场 m = (c, alpha);
      初始化 epsilon ← epsilon_initial;
      初始化经验回放池 buffer_G 和 buffer_M;
  
  For each episode e = 1 to e_max do
      For t = 0 to t_max do
          # 并行博弈阶段
          Parallel For 每个智能体 i=1,2,...,N do
              # 观测与动作选择
              o_G_i(t) = <m_i(t), n_m(t)>;  # 仅与赌场信息相关
              
              # 现在可以计算上个时间步的博弈后的新状态 o_G_next_i(t-1)
              If t > 0 then
                  o_G_next_i(t-1) = o_G_i(t);  # 上次博弈的后继状态
                  # 存储上个时间步的博弈经验 (延迟存储)
                  buffer_G.add( (o_G_i(t-1), a_G_i(t-1), R_i(t-1), o_G_next_i(t-1)) );
              End If
              
              a_G_i(t) = epsilon_greedy(Q_G^i, o_G_i(t), A_G, epsilon);
              # 执行动作并获取奖励
              R_i(t) = 执行博弈动作 a_G_i(t);
          End Parallel
  
          # 并行移动阶段
          Parallel For 每个智能体 i=1,2,...,N do
              # 观测与动作选择
              o_M_i(t) = <m_i(t), n_m(t), n^C_m(t)>;
              
              # 存储上个时间步的移动经验 (延迟存储)
          	If t > 0 then
                      # 添加当前时间步的博弈奖励R_i(t)和观测
                      o_M_next_i(t-1) = o_M_i(t)  
                      buffer_M.add( (o_M_i(t-1), a_M_i(t-1), R_i(t), o_M_next_i(t-1)) );
          	End If
              
              a_M_i(t) = epsilon_greedy(Q_M^i, o_M_i(t), A_M, epsilon);
  
              # 执行移动并更新位置
              m_i(t+1) = 执行移动动作 a_M_i(t);
  
          End Parallel
  
  
          # 经验回放更新 Q_G
          For 每个经验 (o_G, a_G, R, o_G_next) in buffer_G.sample_batch() do
              Target_G = R + gamma * max(Q_G^i(o_G_next, a'_G))
              Q_G^i(o_G, a_G) = (1 - eta) * Q_G^i(o_G, a_G) + eta * Target_G;
          End For
  
          # 经验回放更新 Q_M
          For 每个经验 (o_M, a_M, R, o_M_next) in buffer_M.sample_batch() do
              Target_M = R + gamma * max(Q_M^i(o_M_next, a'_M))
              Q_M^i(o_M, a_M) = (1 - eta) * Q_M^i(o_M, a_M) + eta * Target_M;
          End For
  
          # 动态调整探索率
          epsilon = epsilon * decay_rate
  
          # 清空博弈经验池
          buffer_G.clear()
  
          # 维护移动经验池
          buffer_M.advance_step()  
  
      End For
  End For
  
  ```










————————————————————————————

也可考虑采用一些高级的多智能体强化学习算法，如 DQN，MADDPG，NashQ

1. 如何计算均衡
2. 均衡下的效率度量
3. 机制设计达到好的效率度量

**注意**：在多智能体场景下，环境对于每个智能体而言是动态变化的，因为其他智能体的策略也在不断更新。然而，独立 Q-learning 假设每个智能体的环境是固定的，从而忽略了这种非平稳性。这一假设在实际应用中可能导致收敛性问题，但由于其实现简便和良好的扩展性，独立 Q-learning 仍然是一种常用的基线方法。



————————————————————————————

